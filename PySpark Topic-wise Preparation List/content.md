# ⚡ PySpark Interview Questions – Complete Guide

This document provides a list of frequently asked PySpark interview questions to help you prepare for roles involving big data and distributed computing using Apache Spark with Python.

---

## ❓ PySpark Interview Questions

| Q.No. | Question |
|:-----:|----------|
| 1 | What is PySpark? How is it different from Apache Spark? |
| 2 | Explain the Spark architecture and its main components. |
| 3 | What is an RDD? How do you create one? |
| 4 | What are transformations and actions in PySpark? Give examples. |
| 5 | What is lazy evaluation in PySpark? Why is it important? |
| 6 | How do you create a DataFrame in PySpark? |
| 7 | What is the difference between DataFrame and RDD? |
| 8 | How do you read a CSV file into a DataFrame? |
| 9 | How do you filter rows in a DataFrame? |
| 10 | How do you handle missing values in PySpark DataFrames? |
| 11 | What is a SparkSession? How do you create one? |
| 12 | How do you perform groupBy and aggregation in PySpark? |
| 13 | How do you join two DataFrames in PySpark? |
| 14 | What are broadcast variables? When should you use them? |
| 15 | How do you cache or persist a DataFrame? |
| 16 | What is the difference between cache() and persist()? |
| 17 | How do you optimize a slow-running PySpark job? |
| 18 | What are partitioning and coalesce in PySpark? |
| 19 | How do you write a DataFrame to a Parquet file? |
| 20 | What are UDFs? How do you use them in PySpark? |
| 21 | How do you handle schema changes in PySpark? |
| 22 | What is Spark SQL? How do you run SQL queries on DataFrames? |
| 23 | What is the Catalyst Optimizer? |
| 24 | How do you perform window functions (e.g., rank, row_number) in PySpark? |
| 25 | How do you remove duplicate rows from a DataFrame? |
| 26 | How do you read data from a Hive table in PySpark? |
| 27 | What are the different file formats supported by Spark? |
| 28 | How do you use map and flatMap transformations? |
| 29 | How do you count the number of rows in a DataFrame? |
| 30 | How do you perform pivot operations in PySpark? |
| 31 | How do you handle null values during joins? |
| 32 | What is checkpointing? When do you use it? |
| 33 | How does PySpark ensure fault tolerance? |
| 34 | How do you implement streaming data processing in PySpark? |
| 35 | What is Structured Streaming? |
| 36 | How do you monitor and debug PySpark jobs? |
| 37 | What is Delta Lake? How does it help with data reliability? |
| 38 | Explain a real-time scenario where you optimized a PySpark pipeline. |
| 39 | How do you repartition a DataFrame? |
| 40 | How do you use windowed aggregations in streaming? |
| 41 | How do you implement custom error handling in PySpark jobs? |
| 42 | How do you write a PySpark job to aggregate total sales per region? |
| 43 | How do you fill missing purchase amounts with the average of their category? |
| 44 | How do you use the agg() function in PySpark? |
| 45 | How do you read data from a JSON file and show its schema? |
| 46 | What is the difference between collect() and take()? |
| 47 | How do you use the withColumn method? |
| 48 | How do you use Spark UI to analyze job performance? |
| 49 | What is the role of the cluster manager in Spark? |
| 50 | How do you ensure security and access control in Spark applications? |



